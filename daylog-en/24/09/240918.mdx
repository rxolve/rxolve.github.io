---
title: "Understanding Unicode and Character Sets: What Developers Must Know, Part 2"
date: "240918"
tags: ["Article"]
---

> This is a summary of Chapter 4 from _Joel on Software_, split into two parts.

## Encoding

It's time to talk about encoding.

The early idea behind Unicode encoding, which gave rise to the "2-byte myth," was that you could represent 'Hello' like this:

00 48 00 65 00 6C 00 6C 00 6F

But what about this format?

48 00 65 00 6C 00 6C 00 6F 00

Technically, both are valid. This led to two ways of storing Unicode.

To deal with this, a strange convention emerged where FE FF is stored at the beginning of Unicode strings. This is called the "Unicode byte order mark" (BOM). If the byte order is reversed, it becomes FF FE, meaning every byte in the string has to be reversed.

Of course, programmers started complaining. They had been ignoring Unicode for years, and things were getting worse.

That's when a great concept called UTF-8 was introduced.

UTF-8 is another way to store Unicode characters. It uses 8-bit bytes to store the "magic" U+ numbers.

In UTF-8, all code points from 0 to 127 are stored in 1 byte. Code points above 128 start at 2 or 3 bytes and can go up to 6 bytes.

| Hex Min  | Hex Max  | Binary Byte Representation                   |
| -------- | -------- | -------------------------------------------- |
| 00000000 | 0000007F | 0xxxxxxx                                     |
| 00000080 | 000007FF | 110xxxxx 10xxxxxx                            |
| 00000800 | 0000FFFF | 1110xxxx 10xxxxxx 10xxxxxx                   |
| 00010000 | 001FFFFF | 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx          |
| 00200000 | 03FFFFFF | 111110xx 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx |
| 04000000 | 7FFFFFFF | 1111110x 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx |

Using this method, UTF-8 is fully compatible with ASCII strings and can store Unicode efficiently.

So, the 'Hello' string we saw earlier, represented as U+0065 U+006C U+006C U+006F, would be stored as 48 65 6C 6C 6F, which is exactly the same as how it would be stored in ASCII, ANSI, or any character set used worldwide.

However, only non-English speakers have to deal with all the challenges.

> The book also discusses UCS-2, UTF-16, UTF-7, UCS-4, and Latin-1 (ISO-8859-1), but we'll skip those.

## The Most Important Thing About Encoding

Even if you forget everything else, remember this one thing: A string without its encoding information is meaningless.

**There is no such thing as 'plain text'.**

Whether a string is in memory, a file, or an email, you must know its encoding. Otherwise, you can't interpret it correctly or display it to users properly.

Most of the problems with garbled text occur because the encoding of the string wasn't communicated properly, causing it to display incorrectly or even making it impossible to tell where the string ends.

So how do you store encoding information?

For email messages, it's placed in the header, like this: `Content-Type: text/plain; charset="UTF-8"`.

For web pages, the web server sends an HTTP header similar to this along with the page.

However, there’s a big problem with this approach. Websites are made up of pages written by many people in many languages.

In these cases, the web server has no way of knowing the encoding for each file, so it can only send a general `Content-Type` header for the entire system.

If the browser can’t find `Content-Type` information in the HTTP headers or meta tags, what does it do?

Internet Explorer does something interesting.

It analyzes the byte frequencies that appear when combining certain languages with certain encodings and uses that data to guess the language and encoding.

Since every language has a characteristic pattern in its letter frequencies, this hack often works pretty well.

But if the browser guesses wrong and shows a page in the wrong language, what can the user do?

They can go to the "View Encoding" menu and try switching between different encodings until the text looks right. Of course, some users know how to do this, but most don’t.

UTF-8 has become the preferred encoding for web browsers over the years.

This is how _Joel on Software_ is encoded in 29 different languages, and so far, no one has had trouble reading it.

## What About Chrome?

Since the book was written a long time ago, I was curious how Chrome guesses the encoding today.

It turns out that Chrome hasn’t evolved much beyond Internet Explorer’s guessing logic. Here's what it does:

1. **HTTP Header and Meta Tag Analysis**

   Chrome first checks the HTTP headers sent by the server. If there’s no charset info in the headers, Chrome looks for a `<meta charset="...">` tag in the HTML document.

2. **BOM (Byte Order Mark) Detection**

   If no encoding info is found in the headers or meta tags, Chrome checks the file’s beginning for a BOM. The BOM marks the file’s encoding in formats like UTF-8 and UTF-16.

3. **Histogram Analysis (Fallback)**

   If neither the HTTP headers, meta tags, nor BOM provide encoding information, Chrome falls back to guessing the encoding using byte frequency analysis. Like Internet Explorer, Chrome analyzes byte patterns that are typical for specific encodings and tries to make an educated guess.

> Summarizing this book helped me better understand how strings and encoding systems work.
>
> The encoding guessing methods of Internet Explorer and Chrome are fascinating.
