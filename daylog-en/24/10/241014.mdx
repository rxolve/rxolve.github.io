---
title: "Data Streaming with Query Builder"
date: "241014"
tags: ["NestJS"]
---

Data streaming is a technique that minimizes memory usage while efficiently handling resources like networks and I/O.

When querying large datasets with ORM tools such as TypeORM, loading everything into memory at once can lead to performance and stability issues.

This post introduces streaming with Query Builder and how to apply it effectively.

---

## Why is Data Streaming Important?

### Memory Efficiency

Fetching millions of records from the database at once can put heavy pressure on memory and CPU.

If the server’s memory is limited, loading all the data at once can cause Out of Memory errors.

### Network Transmission Optimization

Instead of processing all data at once, sending small chunks to the client reduces network bottlenecks.  
Clients can process the data in real-time, enhancing the user experience.

### Applications in Large-Scale Services

- Real-time log processing: Analyze millions of logs in real-time or send them to external services.
- Exporting large CSV/JSON files: Stream data to avoid loading everything into memory.
- ETL (Extract, Transform, Load) operations: Streaming is commonly used when migrating data to data warehouses.

---

## Query Builder and Data Streams

TypeORM’s QueryBuilder provides a way to write SQL queries while keeping the convenience of an ORM.

With `.stream()`, you can implement data streaming to process query results sequentially instead of loading them all at once.

---

## How Stream Queries Work

### Regular Queries

- Using `query.getMany()` loads the entire result set into memory.
- If the dataset is too large, it may cause Out of Memory errors.
- Example: Running `SELECT * FROM user` loads millions of rows into memory.

### Stream Queries

- Using `.stream()` retrieves the query result as a cursor from the database.
- Data is processed one row at a time using the `data` event.
- It saves memory and sends data over the network in real-time.

---

## Example: Streaming Data from PostgreSQL/MySQL

```ts
import { createQueryBuilder, getConnection } from "typeorm";
import { User } from "./entity/User"; // Entity representing the User table

async function streamUsers() {
  const connection = getConnection();

  const queryStream = connection
    .createQueryBuilder(User, "user")
    .select(["user.id", "user.name", "user.email"])
    .where("user.isActive = :active", { active: true })
    .stream(); // Stream the query result

  queryStream.on("data", (row) => {
    console.log(row); // Process each row in real-time
  });

  queryStream.on("end", () => {
    console.log("All rows have been streamed.");
  });

  queryStream.on("error", (err) => {
    console.error("Stream error:", err);
  });
}

streamUsers();
```

### Key Points:

- `.stream()`: Retrieves the query result as a `ReadableStream` in Node.js.
- `data` event: Fires whenever new data is available, optimizing memory usage.
- `end` event: Fires after all data has been processed.

---

## Example: Streaming Data to a CSV File

If you need to export large datasets to a CSV file, you can use streaming to write the data sequentially to the file.

```ts
import { createQueryBuilder, getConnection } from "typeorm";
import * as fs from "fs";

async function exportUsersToCSV() {
  const writeStream = fs.createWriteStream("users.csv");
  writeStream.write("id,name,email\n"); // CSV header

  const queryStream = getConnection()
    .createQueryBuilder()
    .select(["id", "name", "email"])
    .from("user", "user")
    .where("isActive = :active", { active: true })
    .stream(); // Retrieve the result as a stream

  queryStream.on("data", (row) => {
    writeStream.write(`${row.id},${row.name},${row.email}\n`); // Write each row to the file
  });

  queryStream.on("end", () => {
    console.log("CSV export completed.");
    writeStream.end();
  });

  queryStream.on("error", (err) => {
    console.error("Stream error:", err);
  });
}

exportUsersToCSV();
```

### Key Points:

- `fs.createWriteStream()`: Streams data to the file in small chunks.
- The database and file operations are asynchronous, preventing network bottlenecks.

---

## Things to Consider When Using Streams

### Compatibility with Transactions

- Stream queries can be used with transactions, but keeping transactions open for too long may cause database locks.

### Error Handling

- Handle `error` events properly to avoid stream interruptions due to network or database errors.

### Memory Management

- While streaming is memory-efficient, processing speed must keep up with the data flow to avoid memory leaks.
- Use `pause()` and `resume()` methods to control the data flow when needed.

---

## Conclusion

The streaming feature in QueryBuilder helps efficiently process large datasets while reducing memory usage and improving real-time response.

This technique is highly useful for ETL pipelines, large-scale log processing, and generating real-time reports.

For developers building high-performance applications, mastering streaming techniques will be valuable in creating stable and scalable systems.
